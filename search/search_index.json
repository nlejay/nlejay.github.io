{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Accueil En plein apprentissage de la Data Science, je pr\u00e9sente \u00e0 travers ce site quelques projets que j'ai r\u00e9alis\u00e9s \u00e0 partir de donn\u00e9es libres d'utilisation (Open Data). J'alimenterai le site au fur et \u00e0 mesure par de nouvelles r\u00e9alisations dans les domaines suivants : Nettoyage, manipulations, visualisation de donn\u00e9es en Python Nettoyage, manipulations, visualisation de donn\u00e9es sous tableur Requ\u00eates en SQL Tableaux de bord sous Power BI ou Tableau Dans chaque section, vous trouverez ma d\u00e9marche ainsi que le travail que j'ai effectu\u00e9 mais je ne pr\u00e9sente pas le code complet. En revanche, vous trouverez un lien vers les fichiers contenant les codes ainsi que les sources de donn\u00e9ees utilis\u00e9es. Vous pouvez me retrouver sur : Github : https://github.com/nlejay Linkedin : https://www.linkedin.com/in/nicolas-lejay-422817164/","title":"Accueil"},{"location":"#accueil","text":"En plein apprentissage de la Data Science, je pr\u00e9sente \u00e0 travers ce site quelques projets que j'ai r\u00e9alis\u00e9s \u00e0 partir de donn\u00e9es libres d'utilisation (Open Data). J'alimenterai le site au fur et \u00e0 mesure par de nouvelles r\u00e9alisations dans les domaines suivants : Nettoyage, manipulations, visualisation de donn\u00e9es en Python Nettoyage, manipulations, visualisation de donn\u00e9es sous tableur Requ\u00eates en SQL Tableaux de bord sous Power BI ou Tableau Dans chaque section, vous trouverez ma d\u00e9marche ainsi que le travail que j'ai effectu\u00e9 mais je ne pr\u00e9sente pas le code complet. En revanche, vous trouverez un lien vers les fichiers contenant les codes ainsi que les sources de donn\u00e9ees utilis\u00e9es. Vous pouvez me retrouver sur : Github : https://github.com/nlejay Linkedin : https://www.linkedin.com/in/nicolas-lejay-422817164/","title":"Accueil"},{"location":"APropos/","text":"Mon parcours J'ai commenc\u00e9 ma carri\u00e8re en tant que professeur de math\u00e9matiques en 2004. J'ai exerc\u00e9 ce m\u00e9tier en coll\u00e8ge et en lyc\u00e9e, en France m\u00e9tropolitaine mais \u00e9galement en Guyane dans un quartier d\u00e9favoris\u00e9 de la banlieue de Cayenne. En 2015, j'ai commenc\u00e9 \u00e0 me former en algorithmique et programmation en Python. Cela m'a permis d'enseigner la sp\u00e9cialit\u00e9 ISN (Informatique et Sciences du Num\u00e9rique) en terminale S de 2015 \u00e0 2019, puis la sp\u00e9cialit\u00e9 NSI (Num\u00e9rique et Sciences Informatiques) en premi\u00e8re et terminale depuis la r\u00e9forme du lyc\u00e9e mise en place en 2019. Aujourd'hui, je suis une formation de l'universit\u00e9 de Cergy pour devenir Data Analyst .","title":"A propos de moi"},{"location":"APropos/#mon-parcours","text":"J'ai commenc\u00e9 ma carri\u00e8re en tant que professeur de math\u00e9matiques en 2004. J'ai exerc\u00e9 ce m\u00e9tier en coll\u00e8ge et en lyc\u00e9e, en France m\u00e9tropolitaine mais \u00e9galement en Guyane dans un quartier d\u00e9favoris\u00e9 de la banlieue de Cayenne. En 2015, j'ai commenc\u00e9 \u00e0 me former en algorithmique et programmation en Python. Cela m'a permis d'enseigner la sp\u00e9cialit\u00e9 ISN (Informatique et Sciences du Num\u00e9rique) en terminale S de 2015 \u00e0 2019, puis la sp\u00e9cialit\u00e9 NSI (Num\u00e9rique et Sciences Informatiques) en premi\u00e8re et terminale depuis la r\u00e9forme du lyc\u00e9e mise en place en 2019. Aujourd'hui, je suis une formation de l'universit\u00e9 de Cergy pour devenir Data Analyst .","title":"Mon parcours"},{"location":"competences/","text":"Comp\u00e9tences pour la Data Science Math\u00e9matiques Ma\u00eetrise des fonctions fondamentales des tableurs Fonctions de calculs Fonctions de recherche, de filtre Tableaux crois\u00e9s dynamiques Repr\u00e9sentations graphiques Algorithmique et Programmation en Python Concepts fondamentaux (structures conditionnelles, boucles, fonctions,...) Strucutures de donn\u00e9es (listes, dictionnaires, tuples, arbres, graphes, piles,...) Les bases de la Programmation Orient\u00e9e Objet Algorithmique (r\u00e9cursivit\u00e9, diviser pour r\u00e9gner, programmation dynamique, algorithmes gloutons,...) Manipulation de donn\u00e9es en Python Nettoyage, manipulation de donn\u00e9es, jointures de dataframes,... (connaissance du module pandas) Visualisation de donn\u00e9es (modules matplotlib, seaborn, Folium) Interrogation de bases de donn\u00e9es en SQL Requ\u00eates classiques (SELECT...FROM...WHERE), sous-requ\u00eates Jointure de tables Common Table Expressions (CTE) Window functions Bases de Machine Learning en Python Module scikit-learn Algorithmes de r\u00e9gression, de classification et de clustering Entrainement et \u00e9valuation des mod\u00e8les (validation crois\u00e9e, recherche de minimisation des probl\u00e8mes d'underfitting et d'overfitting) Notions de bases du logiciel Microsoft Power BI Importation et nettoyage de donn\u00e9es Tableaux de bords Data Analysis Expressions (DAX)","title":"Comp\u00e9tences pour la Data Science"},{"location":"competences/#competences-pour-la-data-science","text":"","title":"Comp\u00e9tences pour la Data Science"},{"location":"competences/#mathematiques","text":"","title":"Math\u00e9matiques"},{"location":"competences/#maitrise-des-fonctions-fondamentales-des-tableurs","text":"Fonctions de calculs Fonctions de recherche, de filtre Tableaux crois\u00e9s dynamiques Repr\u00e9sentations graphiques","title":"Ma\u00eetrise des fonctions fondamentales des tableurs"},{"location":"competences/#algorithmique-et-programmation-en-python","text":"Concepts fondamentaux (structures conditionnelles, boucles, fonctions,...) Strucutures de donn\u00e9es (listes, dictionnaires, tuples, arbres, graphes, piles,...) Les bases de la Programmation Orient\u00e9e Objet Algorithmique (r\u00e9cursivit\u00e9, diviser pour r\u00e9gner, programmation dynamique, algorithmes gloutons,...)","title":"Algorithmique et Programmation en Python"},{"location":"competences/#manipulation-de-donnees-en-python","text":"Nettoyage, manipulation de donn\u00e9es, jointures de dataframes,... (connaissance du module pandas) Visualisation de donn\u00e9es (modules matplotlib, seaborn, Folium)","title":"Manipulation de donn\u00e9es en Python"},{"location":"competences/#interrogation-de-bases-de-donnees-en-sql","text":"Requ\u00eates classiques (SELECT...FROM...WHERE), sous-requ\u00eates Jointure de tables Common Table Expressions (CTE) Window functions","title":"Interrogation de bases de donn\u00e9es en SQL"},{"location":"competences/#bases-de-machine-learning-en-python","text":"Module scikit-learn Algorithmes de r\u00e9gression, de classification et de clustering Entrainement et \u00e9valuation des mod\u00e8les (validation crois\u00e9e, recherche de minimisation des probl\u00e8mes d'underfitting et d'overfitting)","title":"Bases de Machine Learning en Python"},{"location":"competences/#notions-de-bases-du-logiciel-microsoft-power-bi","text":"Importation et nettoyage de donn\u00e9es Tableaux de bords Data Analysis Expressions (DAX)","title":"Notions de bases du logiciel Microsoft Power BI"},{"location":"experience/","text":"Experience professionnelle Enseignant en math\u00e9matiques Lyc\u00e9e Eug\u00e8ne Freyssinet, Saint-Brieuc (22) | 2013-2022 GRETA des C\u00f4tes d'Armor, Saint-Brieuc (22) | 2021-2022 Coll\u00e8ge Lise Ophion, Matoury (Guyane fran\u00e7aise) | 2005-2013 Lyc\u00e9e Aristide Briand, Saint-Nazaire (44) | 2004-2005 Enseignant en Num\u00e9rique et Sciences Informatiques (NSI) Lyc\u00e9e Eug\u00e8ne Freyssinet, Saint-Brieuc (22) | 2019-2022 Enseignant en Informatique et Sciences du Num\u00e9rique (ISN) Lyc\u00e9e Eug\u00e8ne Freyssinet, Saint-Brieuc (22) | 2015-2019","title":"Exp\u00e9rience professionnelle"},{"location":"experience/#experience-professionnelle","text":"","title":"Experience professionnelle"},{"location":"experience/#enseignant-en-mathematiques","text":"Lyc\u00e9e Eug\u00e8ne Freyssinet, Saint-Brieuc (22) | 2013-2022 GRETA des C\u00f4tes d'Armor, Saint-Brieuc (22) | 2021-2022 Coll\u00e8ge Lise Ophion, Matoury (Guyane fran\u00e7aise) | 2005-2013 Lyc\u00e9e Aristide Briand, Saint-Nazaire (44) | 2004-2005","title":"Enseignant en math\u00e9matiques"},{"location":"experience/#enseignant-en-numerique-et-sciences-informatiques-nsi","text":"Lyc\u00e9e Eug\u00e8ne Freyssinet, Saint-Brieuc (22) | 2019-2022","title":"Enseignant en Num\u00e9rique et Sciences Informatiques (NSI)"},{"location":"experience/#enseignant-en-informatique-et-sciences-du-numerique-isn","text":"Lyc\u00e9e Eug\u00e8ne Freyssinet, Saint-Brieuc (22) | 2015-2019","title":"Enseignant en Informatique et Sciences du Num\u00e9rique (ISN)"},{"location":"formation/","text":"Formation Universit\u00e9 de Cergy DU Data Analyst | Mars-d\u00e9cembre 2023 (en cours) Centre National d'Enseignement \u00e0 Distance (CNED) Pr\u00e9paration au CAPES de math\u00e9matiques | 2002-2003 Re\u00e7u au concours du CAPES en 2003 Universit\u00e9 de Rennes 1 DIU Enseigner l'informatique au lyc\u00e9e (Formation continue pour les enseignants) | 2019-2020 Plateforme FUN-MOOC MOOC Python 3 : des fondamentaux aux concepts avanc\u00e9s | 2018 MOOC Machine learning in Python with scikit-learn | Nov 2022-Janv 2023 Plateforme Datacamp Cours de traitement de donn\u00e9es en Python Importing and cleaning data with Python Data Manipulation with Python Data Visualization with Python Cours de traitement de donn\u00e9es avec tableur Spreadsheet fundamentals Cours de requ\u00eates de bases de donn\u00e9es SQL fundamentals Cours d'utilisation du logiciel Microsoft Power BI Power BI fundamentals Institut Catholique des Arts et M\u00e9tiers de Lille (ICAM) Dipl\u00f4me d'ing\u00e9nieur | 1996-2001","title":"Formation/Dipl\u00f4mes"},{"location":"formation/#formation","text":"","title":"Formation"},{"location":"formation/#universite-de-cergy","text":"DU Data Analyst | Mars-d\u00e9cembre 2023 (en cours)","title":"Universit\u00e9 de Cergy"},{"location":"formation/#centre-national-denseignement-a-distance-cned","text":"Pr\u00e9paration au CAPES de math\u00e9matiques | 2002-2003 Re\u00e7u au concours du CAPES en 2003","title":"Centre National d'Enseignement \u00e0 Distance (CNED)"},{"location":"formation/#universite-de-rennes-1","text":"DIU Enseigner l'informatique au lyc\u00e9e (Formation continue pour les enseignants) | 2019-2020","title":"Universit\u00e9 de Rennes 1"},{"location":"formation/#plateforme-fun-mooc","text":"MOOC Python 3 : des fondamentaux aux concepts avanc\u00e9s | 2018 MOOC Machine learning in Python with scikit-learn | Nov 2022-Janv 2023","title":"Plateforme FUN-MOOC"},{"location":"formation/#plateforme-datacamp","text":"Cours de traitement de donn\u00e9es en Python Importing and cleaning data with Python Data Manipulation with Python Data Visualization with Python Cours de traitement de donn\u00e9es avec tableur Spreadsheet fundamentals Cours de requ\u00eates de bases de donn\u00e9es SQL fundamentals Cours d'utilisation du logiciel Microsoft Power BI Power BI fundamentals","title":"Plateforme Datacamp"},{"location":"formation/#institut-catholique-des-arts-et-metiers-de-lille-icam","text":"Dipl\u00f4me d'ing\u00e9nieur | 1996-2001","title":"Institut Catholique des Arts et M\u00e9tiers de Lille (ICAM)"},{"location":"projets/ips/","text":"Indice de Position Sociale (IPS) des lyc\u00e9es fran\u00e7ais Comp\u00e9tences mises en \u0153uvre Nettoyage, validation de donn\u00e9es Visualisation de donn\u00e9es (histogrammes, nuages de points, diagrammes en barres) Fonctions d'agr\u00e9gation classiques Tableaux crois\u00e9s dynamiques Fonctions avanc\u00e9es ( XLOOKUP , FILTER ,...) Tableau de bord int\u00e9ractif Probl\u00e9matique D\u00e9but 2023, le minist\u00e8re de l'\u00e9ducation nationale a d\u00e9voil\u00e9 l'Indice de Position Sociale des lyc\u00e9es fran\u00e7ais. Cet indice permet d'attribuer une valeur quantitative \u00e0 l'origine sociale des \u00e9l\u00e8ves. Plus l'indice est grand, plus l'\u00e9l\u00e8ve est issu d'une cat\u00e9gorie sociale consid\u00e9r\u00e9e comme favorable \u00e0 sa r\u00e9ussite scolaire. Cet indicateur est calcul\u00e9 \u00e0 partir de plusieurs crit\u00e8res : dipl\u00f4mes des parents, conditions mat\u00e9rielles dans le foyer (nombre de pi\u00e8ces du logement, chambre seul, ordinateur,...), revenus des parents,... L'IPS attribu\u00e9 \u00e0 un lyc\u00e9e est une moyenne des IPS des \u00e9l\u00e8ves de l'\u00e9tablissement. Le but de ce petit projet \u00e9tait d'utiliser les fonctionnalit\u00e9s d'un tableur pour explorer le jeu de donn\u00e9es et en tirer des informations \u00e0 partir de calculs et de repr\u00e9sentations graphiques. Je tiens \u00e0 pr\u00e9ciser que je ne fais ici que des constats et qu'aucun jugement sur les r\u00e9sultats ne sera prononc\u00e9. Le fichier contenant le travail effectu\u00e9 est disponible ici au format xlsx. Pr\u00e9sentation du jeu de donn\u00e9es Les donn\u00e9es repr\u00e9sentent la situation des lyc\u00e9es lors de l'ann\u00e9e scolaire 2021-2022. Le jeu de donn\u00e9es est constitu\u00e9 de 13 colonnes dont voici une description pour les plus importantes : academie : academie dans laquelle se situe le lyc\u00e9e uai : code UAI de l'\u00e9tablissement (permet d'identifier l'\u00e9tablissement de mani\u00e8re unique) nom_de_l_\u00e9tablissement nom_de_la_commune secteur : indique si l'\u00e9tablissement est public ou priv\u00e9 sous contrat type_de_lycee : indique si l'\u00e9tablissement est un LEGT (Lyc\u00e9e d'Enseignement G\u00e9n\u00e9ral et Technologique), un LP (Lyc\u00e9e Professionnel) ou un LPO (Lyc\u00e9e Polyvalent qui regroupe une section GT et une section professionnelle) ips_voie_gt : IPS de l'\u00e9tablissement si c'est un LEGT, IPS de la seule section GT si c'est un LPO, vide si c'est un LP ips_voie_pro : IPS de l'\u00e9tablissement si c'est un LP, IPS de la seule section Pro si c'est un LPO, vide si c'est un LEGT ips_ensemble_gt_pro : ips de l'ensemble de l'\u00e9tablissement Premiers r\u00e9sum\u00e9s statistiques Des premiers param\u00e8tres statistiques sont faciles \u00e0 obtenir \u00e0 l'aide des fonctions AVERAGE , STDEV , MEDIAN , ou encore MIN et MAX . On obtient les r\u00e9sultats suivants : l'IPS moyen est de 103,91 avec un \u00e9cart-type de 18,9 ; l'IPS m\u00e9dian est de 103,8 ; l'IPS le plus petit est de 49,5 et le plus grand est de 159. On remarque que la moyenne et la m\u00e9diane sont proches, ce qui am\u00e8ne \u00e0 penser \u00e0 une certaine sym\u00e9trie de la r\u00e9partition des \u00e9tablissements selon leur IPS, ce qui est confirm\u00e9 en tra\u00e7ant un histogramme \u00e0 l'aide du tableur. Comparaison des \u00e9tablissements suivant leur type Les param\u00e8tres pr\u00e9c\u00e9dents n'ont qu'un int\u00e9r\u00eat limit\u00e9 pour comprendre le jeu de donn\u00e9es. On peut pousser l'analyse plus loin en comparant les \u00e9tablissements en fonction de leur type. On peut calculer les m\u00eames param\u00e8tres mais cette fois pour chaque type d'\u00e9tablissement (LGT, LP, LPO). J'ai utilis\u00e9 pour cela une pivot table (tableau crois\u00e9 dynamique). On s'aper\u00e7oit alors que les LGT accueillent en moyenne des \u00e9l\u00e8ves plus favoris\u00e9s socialement que les LP. Il existe n\u00e9anmoins des LGT \u00e0 IPS faible et des LP \u00e0 IPS \u00e9lev\u00e9. Cela n'est pas pr\u00e9sent\u00e9 dans le tableau ci-dessus mais la position interm\u00e9diaire des LPO est due au fait qu'ils comportent \u00e0 la fois une section GT et une section Pro. Leur section GT accueille en moyenne des \u00e9l\u00e8ves \u00e0 IPS plus \u00e9lev\u00e9s que ceux de leur section Pro (108,11 contre 88,29). Pour confirmer cet \u00e9cart entre les LGT et les LP, regardons la r\u00e9partition des lyc\u00e9es en fonction de leur type (dans le diagramme ci-dessous, les LPO sont inclus mais leur section GT a \u00e9t\u00e9 incluse aux donn\u00e9es des LGT et leur section Pro aux donn\u00e9es des LP). Le diagramme montre clairement la diff\u00e9rence des populations des LGT et des LP. Remarque technique Pour r\u00e9aliser cet histogramme, il faut utiliser deux s\u00e9ries de donn\u00e9es : une pour chaque section. Cela est facilit\u00e9 ici par le fait qu'il existe d\u00e9j\u00e0 une colonne sp\u00e9cifique regroupant l'IPS des sections GT et une autre pour les sections Pro. Si cela n'avait pas \u00e9t\u00e9 le cas, il aurait \u00e9t\u00e9 prossible de faire un tri des donn\u00e9es selon le type de l'\u00e9tablissement. J'ai \u00e9galement calcul\u00e9 la proportion des \u00e9tablissements de chaque type dont l'IPS est sup\u00e9rieur ou \u00e9gal \u00e0 la moyenne de l'ensemble des \u00e9tablissements : On s'aper\u00e7oit que 84,25 % des LGT ont un IPS sup\u00e9rieur ou \u00e9gal \u00e0 l'IPS moyen, ce qui n'est le cas que pour 6,63 % des LP. Remarque technique Pour r\u00e9aliser ce calcul, il faut compter le nombre de lyc\u00e9es qui sont des LGT et qui ont un IPS sup\u00e9rieur \u00e0 la moyenne afin de diviser le r\u00e9sultat par le nombre total de LGT. Cela se fait facilement \u00e0 l'aide de la fonction COUNTIFS ( NB.SI.ENS en fran\u00e7ais). On peut faire le m\u00eame type de comparaison sur les 100 \u00e9tablissements aux IPS les plus \u00e9lev\u00e9s. On trouve alors que ce sont tous des LGT ! Concernant les 100 \u00e9tablissements aux IPS les moins \u00e9lev\u00e9s, on trouve 75 LP, 21 LPO et seulement 4 LGT. Comparaison des \u00e9tablissements suivant leur secteur Il est aussi int\u00e9ressant de comparer les secteurs publics et priv\u00e9s. L\u00e0-aussi, \u00e0 l'aide d'une pivot table , on obtient les r\u00e9sultats suivants : On s'aper\u00e7oit qu'en moyenne les lyc\u00e9es priv\u00e9s poss\u00e8dent des \u00e9l\u00e8ves plus favoris\u00e9s socialement. Mais on remarquera n\u00e9anmoins que l'IPS minimal est celui d'un lyc\u00e9e priv\u00e9 et que l'IPS maximal est celui d'un lyc\u00e9e public. Cependant, ces derniers font figures d'exception. En effet, lorsque l'on regarde les 100 \u00e9tablissements les plus favoris\u00e9s, 80 sont priv\u00e9s et seulement 20 sont publics. Du c\u00f4t\u00e9 des 100 \u00e9tablissements aux \u00e9l\u00e8ves les moins favoris\u00e9s, 84 sont publics et seulement 16 sont priv\u00e9s. De m\u00eame, 65,8 % des lyc\u00e9es priv\u00e9s ont un IPS sup\u00e9rieur \u00e0 la moyenne alors que ce n'est le cas que de 41,5 % des \u00e9tablissements publics. Voici la r\u00e9partition des lyc\u00e9es suivant leur IPS en fonction de leur secteur : Pour terminer cette comparaison, on peut voir sur le diagramme ci-dessous que quel que soit le type d'\u00e9tablissement, le secteur priv\u00e9 accueille en moyenne des \u00e9l\u00e8ves plus favoris\u00e9s que le secteur public. Remarque technique Pour r\u00e9aliser les calculs et les diagrammes de cette partie, j'ai utilis\u00e9 les m\u00eames m\u00e9thodes que pour la partie pr\u00e9c\u00e9dente. La seule chose qui change est le fait de r\u00e9partir les donn\u00e9es suivant le secteur plut\u00f4t que le type. Corr\u00e9lation entre IPS et r\u00e9ussite au bac ? On peut se poser la question : y-a-t-il une meilleure r\u00e9ussite chez les \u00e9l\u00e8ves dont l'IPS est le plus grand ? Regardons s'il existe une corr\u00e9lation entre le taux de r\u00e9ussite au bac des lyc\u00e9es et leur IPS. Pour cela, j'ai utilis\u00e9 un deuxi\u00e8me jeu de donn\u00e9es disponible sur le site data.education.gouv.fr . On trouve dans ce jeu de donn\u00e9es le nom et le code UAI des lyc\u00e9es de type GT ainsi qu'un certain nombre de variables dont celle qui nous int\u00e9resse : le taux de r\u00e9ussite au baccalaur\u00e9at en 2021. On peut alors tracer un nuage de points repr\u00e9sentant le taux de r\u00e9ussite au bac en fonction de l'IPS des lyc\u00e9es (la droite rouge est la droite de r\u00e9gression lin\u00e9aire). Remarque technique Pour juxtaposer l'IPS et le taux de r\u00e9ussite au bac d'un m\u00eame \u00e9tablissement, il faut mettre en relation les deux jeux de donn\u00e9es (faire une sorte de jointure de tables). On peut par exemple partir du jeu de donn\u00e9es sur le taux de r\u00e9ussite au bac et aller chercher pour chaque \u00e9tablissement l'IPS correspondant. J'ai utilis\u00e9 pour cela la fonction XLOOKUP qui m'a permis de faire une recherche de l'IPS d'un \u00e9tablissement \u00e0 partir de son code UAI. On peut remarquer que dans l'ensemble le taux de r\u00e9ussite est meilleur lorsque l'IPS devient grand, m\u00eame s'il existe de tr\u00e8s bons taux de r\u00e9ussite dans des lyc\u00e9es avec un IPS moyen voire faible. Cependant, le coefficient de corr\u00e9lation lin\u00e9aire entre les deux variables n'est que de 0,55. La corr\u00e9lation est donc assez moyenne mais pas inexistante (corr\u00e9lation lin\u00e9aire). On peut d'ailleurs remarquer que le taux de r\u00e9ussite moyen des \u00e9tablissements dont l'IPS est inf\u00e9rieur \u00e0 100 est de 94 % contre 98 % pour ceux dont l'IPS est sup\u00e9rieur ou \u00e9gal \u00e0 100. De m\u00eame, 86 % des lyc\u00e9es \u00e0 IPS inf\u00e9rieur \u00e0 100 ont un taux de r\u00e9ussite sup\u00e9rieur ou \u00e9gal \u00e0 90 % alors qu'ils repr\u00e9sentent 99 % des lyc\u00e9es \u00e0 IPS sup\u00e9rieur ou \u00e9gal \u00e0 100. On peut tout de m\u00eame \u00e9mettre des r\u00e9serves sur cette analyse : En 2021, la crise du covid-19 \u00e9tait encore bien pr\u00e9sente. Beaucoup de lyc\u00e9es ont utilis\u00e9 un fonctionnement hybride pr\u00e9sentiel-distanciel pendant une bonne partie de l'ann\u00e9e scolaire. Pour tenir compte des difficult\u00e9s d'apprentissage des \u00e9l\u00e8ves, les \u00e9preuves du bac ont \u00e9t\u00e9 am\u00e9nag\u00e9es. Les taux de r\u00e9ussite sont donc \u00e0 prendre avec du recul. Depuis la r\u00e9forme du lyc\u00e9e initi\u00e9e en 2019, le contr\u00f4le continu a pris une grande place dans le r\u00e9sultats du bac (40 % de la note finale). On peut se demander si tous les \u00e9tablissements jouent le jeu ou si certains ont tendance \u00e0 gonfler les notes pour obtenir de meilleurs r\u00e9sultats au bac. Est-on not\u00e9 de la m\u00eame mani\u00e8re dans tous les lyc\u00e9es de France ? Tableau de bord interactif Pour permettre d'analyser les donn\u00e9es pour chaque acad\u00e9mie, j'ai choisi de r\u00e9aliser un tableau de bord interactif. Celui-ci est compos\u00e9 d'un menu d\u00e9roulant permettant de choisir une acad\u00e9mie. La fonction FILTER permet alors de ne faire appara\u00eetre que les donn\u00e9es des lyc\u00e9es de cette acad\u00e9mie. Deux tableaux (IPS par secteur et IPS par type de lyc\u00e9e) ainsi que les deux diagrammes correspondants s'actualisent alors automatiquement.","title":"Indice de Position Sociale des lyc\u00e9es"},{"location":"projets/ips/#indice-de-position-sociale-ips-des-lycees-francais","text":"","title":"Indice de Position Sociale (IPS) des lyc\u00e9es fran\u00e7ais"},{"location":"projets/ips/#competences-mises-en-uvre","text":"Nettoyage, validation de donn\u00e9es Visualisation de donn\u00e9es (histogrammes, nuages de points, diagrammes en barres) Fonctions d'agr\u00e9gation classiques Tableaux crois\u00e9s dynamiques Fonctions avanc\u00e9es ( XLOOKUP , FILTER ,...) Tableau de bord int\u00e9ractif","title":"Comp\u00e9tences mises en \u0153uvre"},{"location":"projets/ips/#problematique","text":"D\u00e9but 2023, le minist\u00e8re de l'\u00e9ducation nationale a d\u00e9voil\u00e9 l'Indice de Position Sociale des lyc\u00e9es fran\u00e7ais. Cet indice permet d'attribuer une valeur quantitative \u00e0 l'origine sociale des \u00e9l\u00e8ves. Plus l'indice est grand, plus l'\u00e9l\u00e8ve est issu d'une cat\u00e9gorie sociale consid\u00e9r\u00e9e comme favorable \u00e0 sa r\u00e9ussite scolaire. Cet indicateur est calcul\u00e9 \u00e0 partir de plusieurs crit\u00e8res : dipl\u00f4mes des parents, conditions mat\u00e9rielles dans le foyer (nombre de pi\u00e8ces du logement, chambre seul, ordinateur,...), revenus des parents,... L'IPS attribu\u00e9 \u00e0 un lyc\u00e9e est une moyenne des IPS des \u00e9l\u00e8ves de l'\u00e9tablissement. Le but de ce petit projet \u00e9tait d'utiliser les fonctionnalit\u00e9s d'un tableur pour explorer le jeu de donn\u00e9es et en tirer des informations \u00e0 partir de calculs et de repr\u00e9sentations graphiques. Je tiens \u00e0 pr\u00e9ciser que je ne fais ici que des constats et qu'aucun jugement sur les r\u00e9sultats ne sera prononc\u00e9. Le fichier contenant le travail effectu\u00e9 est disponible ici au format xlsx.","title":"Probl\u00e9matique"},{"location":"projets/ips/#presentation-du-jeu-de-donnees","text":"Les donn\u00e9es repr\u00e9sentent la situation des lyc\u00e9es lors de l'ann\u00e9e scolaire 2021-2022. Le jeu de donn\u00e9es est constitu\u00e9 de 13 colonnes dont voici une description pour les plus importantes : academie : academie dans laquelle se situe le lyc\u00e9e uai : code UAI de l'\u00e9tablissement (permet d'identifier l'\u00e9tablissement de mani\u00e8re unique) nom_de_l_\u00e9tablissement nom_de_la_commune secteur : indique si l'\u00e9tablissement est public ou priv\u00e9 sous contrat type_de_lycee : indique si l'\u00e9tablissement est un LEGT (Lyc\u00e9e d'Enseignement G\u00e9n\u00e9ral et Technologique), un LP (Lyc\u00e9e Professionnel) ou un LPO (Lyc\u00e9e Polyvalent qui regroupe une section GT et une section professionnelle) ips_voie_gt : IPS de l'\u00e9tablissement si c'est un LEGT, IPS de la seule section GT si c'est un LPO, vide si c'est un LP ips_voie_pro : IPS de l'\u00e9tablissement si c'est un LP, IPS de la seule section Pro si c'est un LPO, vide si c'est un LEGT ips_ensemble_gt_pro : ips de l'ensemble de l'\u00e9tablissement","title":"Pr\u00e9sentation du jeu de donn\u00e9es"},{"location":"projets/ips/#premiers-resumes-statistiques","text":"Des premiers param\u00e8tres statistiques sont faciles \u00e0 obtenir \u00e0 l'aide des fonctions AVERAGE , STDEV , MEDIAN , ou encore MIN et MAX . On obtient les r\u00e9sultats suivants : l'IPS moyen est de 103,91 avec un \u00e9cart-type de 18,9 ; l'IPS m\u00e9dian est de 103,8 ; l'IPS le plus petit est de 49,5 et le plus grand est de 159. On remarque que la moyenne et la m\u00e9diane sont proches, ce qui am\u00e8ne \u00e0 penser \u00e0 une certaine sym\u00e9trie de la r\u00e9partition des \u00e9tablissements selon leur IPS, ce qui est confirm\u00e9 en tra\u00e7ant un histogramme \u00e0 l'aide du tableur.","title":"Premiers r\u00e9sum\u00e9s statistiques"},{"location":"projets/ips/#comparaison-des-etablissements-suivant-leur-type","text":"Les param\u00e8tres pr\u00e9c\u00e9dents n'ont qu'un int\u00e9r\u00eat limit\u00e9 pour comprendre le jeu de donn\u00e9es. On peut pousser l'analyse plus loin en comparant les \u00e9tablissements en fonction de leur type. On peut calculer les m\u00eames param\u00e8tres mais cette fois pour chaque type d'\u00e9tablissement (LGT, LP, LPO). J'ai utilis\u00e9 pour cela une pivot table (tableau crois\u00e9 dynamique). On s'aper\u00e7oit alors que les LGT accueillent en moyenne des \u00e9l\u00e8ves plus favoris\u00e9s socialement que les LP. Il existe n\u00e9anmoins des LGT \u00e0 IPS faible et des LP \u00e0 IPS \u00e9lev\u00e9. Cela n'est pas pr\u00e9sent\u00e9 dans le tableau ci-dessus mais la position interm\u00e9diaire des LPO est due au fait qu'ils comportent \u00e0 la fois une section GT et une section Pro. Leur section GT accueille en moyenne des \u00e9l\u00e8ves \u00e0 IPS plus \u00e9lev\u00e9s que ceux de leur section Pro (108,11 contre 88,29). Pour confirmer cet \u00e9cart entre les LGT et les LP, regardons la r\u00e9partition des lyc\u00e9es en fonction de leur type (dans le diagramme ci-dessous, les LPO sont inclus mais leur section GT a \u00e9t\u00e9 incluse aux donn\u00e9es des LGT et leur section Pro aux donn\u00e9es des LP). Le diagramme montre clairement la diff\u00e9rence des populations des LGT et des LP. Remarque technique Pour r\u00e9aliser cet histogramme, il faut utiliser deux s\u00e9ries de donn\u00e9es : une pour chaque section. Cela est facilit\u00e9 ici par le fait qu'il existe d\u00e9j\u00e0 une colonne sp\u00e9cifique regroupant l'IPS des sections GT et une autre pour les sections Pro. Si cela n'avait pas \u00e9t\u00e9 le cas, il aurait \u00e9t\u00e9 prossible de faire un tri des donn\u00e9es selon le type de l'\u00e9tablissement. J'ai \u00e9galement calcul\u00e9 la proportion des \u00e9tablissements de chaque type dont l'IPS est sup\u00e9rieur ou \u00e9gal \u00e0 la moyenne de l'ensemble des \u00e9tablissements : On s'aper\u00e7oit que 84,25 % des LGT ont un IPS sup\u00e9rieur ou \u00e9gal \u00e0 l'IPS moyen, ce qui n'est le cas que pour 6,63 % des LP. Remarque technique Pour r\u00e9aliser ce calcul, il faut compter le nombre de lyc\u00e9es qui sont des LGT et qui ont un IPS sup\u00e9rieur \u00e0 la moyenne afin de diviser le r\u00e9sultat par le nombre total de LGT. Cela se fait facilement \u00e0 l'aide de la fonction COUNTIFS ( NB.SI.ENS en fran\u00e7ais). On peut faire le m\u00eame type de comparaison sur les 100 \u00e9tablissements aux IPS les plus \u00e9lev\u00e9s. On trouve alors que ce sont tous des LGT ! Concernant les 100 \u00e9tablissements aux IPS les moins \u00e9lev\u00e9s, on trouve 75 LP, 21 LPO et seulement 4 LGT.","title":"Comparaison des \u00e9tablissements suivant leur type"},{"location":"projets/ips/#comparaison-des-etablissements-suivant-leur-secteur","text":"Il est aussi int\u00e9ressant de comparer les secteurs publics et priv\u00e9s. L\u00e0-aussi, \u00e0 l'aide d'une pivot table , on obtient les r\u00e9sultats suivants : On s'aper\u00e7oit qu'en moyenne les lyc\u00e9es priv\u00e9s poss\u00e8dent des \u00e9l\u00e8ves plus favoris\u00e9s socialement. Mais on remarquera n\u00e9anmoins que l'IPS minimal est celui d'un lyc\u00e9e priv\u00e9 et que l'IPS maximal est celui d'un lyc\u00e9e public. Cependant, ces derniers font figures d'exception. En effet, lorsque l'on regarde les 100 \u00e9tablissements les plus favoris\u00e9s, 80 sont priv\u00e9s et seulement 20 sont publics. Du c\u00f4t\u00e9 des 100 \u00e9tablissements aux \u00e9l\u00e8ves les moins favoris\u00e9s, 84 sont publics et seulement 16 sont priv\u00e9s. De m\u00eame, 65,8 % des lyc\u00e9es priv\u00e9s ont un IPS sup\u00e9rieur \u00e0 la moyenne alors que ce n'est le cas que de 41,5 % des \u00e9tablissements publics. Voici la r\u00e9partition des lyc\u00e9es suivant leur IPS en fonction de leur secteur : Pour terminer cette comparaison, on peut voir sur le diagramme ci-dessous que quel que soit le type d'\u00e9tablissement, le secteur priv\u00e9 accueille en moyenne des \u00e9l\u00e8ves plus favoris\u00e9s que le secteur public. Remarque technique Pour r\u00e9aliser les calculs et les diagrammes de cette partie, j'ai utilis\u00e9 les m\u00eames m\u00e9thodes que pour la partie pr\u00e9c\u00e9dente. La seule chose qui change est le fait de r\u00e9partir les donn\u00e9es suivant le secteur plut\u00f4t que le type.","title":"Comparaison des \u00e9tablissements suivant leur secteur"},{"location":"projets/ips/#correlation-entre-ips-et-reussite-au-bac","text":"On peut se poser la question : y-a-t-il une meilleure r\u00e9ussite chez les \u00e9l\u00e8ves dont l'IPS est le plus grand ? Regardons s'il existe une corr\u00e9lation entre le taux de r\u00e9ussite au bac des lyc\u00e9es et leur IPS. Pour cela, j'ai utilis\u00e9 un deuxi\u00e8me jeu de donn\u00e9es disponible sur le site data.education.gouv.fr . On trouve dans ce jeu de donn\u00e9es le nom et le code UAI des lyc\u00e9es de type GT ainsi qu'un certain nombre de variables dont celle qui nous int\u00e9resse : le taux de r\u00e9ussite au baccalaur\u00e9at en 2021. On peut alors tracer un nuage de points repr\u00e9sentant le taux de r\u00e9ussite au bac en fonction de l'IPS des lyc\u00e9es (la droite rouge est la droite de r\u00e9gression lin\u00e9aire). Remarque technique Pour juxtaposer l'IPS et le taux de r\u00e9ussite au bac d'un m\u00eame \u00e9tablissement, il faut mettre en relation les deux jeux de donn\u00e9es (faire une sorte de jointure de tables). On peut par exemple partir du jeu de donn\u00e9es sur le taux de r\u00e9ussite au bac et aller chercher pour chaque \u00e9tablissement l'IPS correspondant. J'ai utilis\u00e9 pour cela la fonction XLOOKUP qui m'a permis de faire une recherche de l'IPS d'un \u00e9tablissement \u00e0 partir de son code UAI. On peut remarquer que dans l'ensemble le taux de r\u00e9ussite est meilleur lorsque l'IPS devient grand, m\u00eame s'il existe de tr\u00e8s bons taux de r\u00e9ussite dans des lyc\u00e9es avec un IPS moyen voire faible. Cependant, le coefficient de corr\u00e9lation lin\u00e9aire entre les deux variables n'est que de 0,55. La corr\u00e9lation est donc assez moyenne mais pas inexistante (corr\u00e9lation lin\u00e9aire). On peut d'ailleurs remarquer que le taux de r\u00e9ussite moyen des \u00e9tablissements dont l'IPS est inf\u00e9rieur \u00e0 100 est de 94 % contre 98 % pour ceux dont l'IPS est sup\u00e9rieur ou \u00e9gal \u00e0 100. De m\u00eame, 86 % des lyc\u00e9es \u00e0 IPS inf\u00e9rieur \u00e0 100 ont un taux de r\u00e9ussite sup\u00e9rieur ou \u00e9gal \u00e0 90 % alors qu'ils repr\u00e9sentent 99 % des lyc\u00e9es \u00e0 IPS sup\u00e9rieur ou \u00e9gal \u00e0 100. On peut tout de m\u00eame \u00e9mettre des r\u00e9serves sur cette analyse : En 2021, la crise du covid-19 \u00e9tait encore bien pr\u00e9sente. Beaucoup de lyc\u00e9es ont utilis\u00e9 un fonctionnement hybride pr\u00e9sentiel-distanciel pendant une bonne partie de l'ann\u00e9e scolaire. Pour tenir compte des difficult\u00e9s d'apprentissage des \u00e9l\u00e8ves, les \u00e9preuves du bac ont \u00e9t\u00e9 am\u00e9nag\u00e9es. Les taux de r\u00e9ussite sont donc \u00e0 prendre avec du recul. Depuis la r\u00e9forme du lyc\u00e9e initi\u00e9e en 2019, le contr\u00f4le continu a pris une grande place dans le r\u00e9sultats du bac (40 % de la note finale). On peut se demander si tous les \u00e9tablissements jouent le jeu ou si certains ont tendance \u00e0 gonfler les notes pour obtenir de meilleurs r\u00e9sultats au bac. Est-on not\u00e9 de la m\u00eame mani\u00e8re dans tous les lyc\u00e9es de France ?","title":"Corr\u00e9lation entre IPS et r\u00e9ussite au bac ?"},{"location":"projets/ips/#tableau-de-bord-interactif","text":"Pour permettre d'analyser les donn\u00e9es pour chaque acad\u00e9mie, j'ai choisi de r\u00e9aliser un tableau de bord interactif. Celui-ci est compos\u00e9 d'un menu d\u00e9roulant permettant de choisir une acad\u00e9mie. La fonction FILTER permet alors de ne faire appara\u00eetre que les donn\u00e9es des lyc\u00e9es de cette acad\u00e9mie. Deux tableaux (IPS par secteur et IPS par type de lyc\u00e9e) ainsi que les deux diagrammes correspondants s'actualisent alors automatiquement.","title":"Tableau de bord interactif"},{"location":"projets/medecins/","text":"Cartographie de la densit\u00e9 de m\u00e9decins g\u00e9n\u00e9ralistes par bassin de vie Comp\u00e9tences mises en \u0153uvre Nettoyage de donn\u00e9es Manipulation de donn\u00e9es Jointures de tables (classiques et spatiales) Visualisation de donn\u00e9es (carte choropl\u00e8the) Probl\u00e9matique L'id\u00e9e de ce projet \u00e9tait d'\u00e9tablir en Python une carte choropl\u00e8the indiquant la densit\u00e9 de m\u00e9decins g\u00e9n\u00e9ralistes par bassin de vie en France m\u00e9tropolitaine. Cette page est un r\u00e9sum\u00e9 du travail effectu\u00e9. Le notebook contenant l'ensemble du code se trouve ici. Donn\u00e9es utilis\u00e9es un fichier .xlsx regroupant le nombre de m\u00e9decins g\u00e9n\u00e9ralistes par commune (source : Observatoire des territoires / ann\u00e9e 2020) un fichier .csv contenant la population de chaque commune au recensement de 2019 (source : INSEE) un fichier .geojson regroupant les bassins de vie ainsi que leurs contours g\u00e9ographiques (source : INSEE) un fichier .geojson des diff\u00e9rentes communes fran\u00e7aises avec leurs donn\u00e9es g\u00e9ographiques (source : https://github.com/gregoiredavid/france-geojson ) On peut voir que les donn\u00e9es sur la population et le nombre de m\u00e9decins ne sont pas de la m\u00eame ann\u00e9e. Mais \u00e0 une ann\u00e9e pr\u00e8s, il y a peu de chances qu'il y ait eu une forte variation de ces donn\u00e9es. De m\u00eame en 2022 la population et le nombre de m\u00e9decins des communes a sans-doute un peu chang\u00e9 mais le r\u00e9sultat obtenu pour l'ann\u00e9e 2020 donne une bonne id\u00e9e de la densit\u00e9 de m\u00e9decins par bassin de vie aujourd'hui en 2022. Modules utilis\u00e9s Comme dit aupravant, j'ai utilis\u00e9 le langage Python pour r\u00e9aliser cette carte. Voici les modules que j'ai utilis\u00e9s : le module pandas pour lire les fichiers csv et xlsx, convertir les donn\u00e9es en dataframes et faire des jointures de ces dataframes le module geopandas pour lire les fichiers geojson, convertir les donn\u00e9es en GeoDataFrames ainsi que pour faire des jointures spatiales le module matplotlib pour la cr\u00e9ation de la carte Nettoyage des donn\u00e9es La premi\u00e8re op\u00e9ration a \u00e9t\u00e9 de nettoyer les donn\u00e9es. Je ne pr\u00e9senterai pas le code correspondant \u00e0 cette partie ici (se r\u00e9f\u00e9rer au notebook ). Voici ce que j'ai pu observer et faire : Il n'y avait pas de donn\u00e9es manquantes dans les fichiers. Je n'ai rien eu \u00e0 faire de ce c\u00f4t\u00e9. Il n'y avait pas de donn\u00e9es en doublons. Il n'y avait pas de donn\u00e9es abb\u00e9rantes. En revanche, la population de chaque commune \u00e9tait au format str . J'ai donc converti cette variable au format int . Le nombre de communes n'\u00e9tait pas le m\u00eame dans chaque fichier. Apr\u00e8s inspection, je me suis rendu compte que certains fichiers ne contenaient que les donn\u00e9es de France m\u00e9tropolitaine alors que d'autres contenaient aussi des donn\u00e9es de communes d'outre-mer. De plus, dans le fichier sur les contours g\u00e9ographiques des communes, les communes de Paris, Lyon et Marseille \u00e9taient divis\u00e9es selon leurs arrondissements. Il a donc fallu faire en sorte qu'il n'y ait plus qu'une ligne pour chacune de ces villes. Dans le cas contraire, cela aurait pos\u00e9 probl\u00e8me lors des jointures avec les autres tables. J'ai enfin supprim\u00e9 des colonnes dont je n'avais pas besoin dans les dataframes. Jointure des tables Arriv\u00e9 \u00e0 ce stade, les donn\u00e9es utiles se trouvent dans quatre dataframes diff\u00e9rents : pop_com pr\u00e9sentant la population des communes medecins pr\u00e9sentant le nombre de m\u00e9decins par commune communes regroupant les contours g\u00e9ographiques des communes bassins regroupant les contours g\u00e9ographiques des bassins de vie J'ai commenc\u00e9 par faire une jointure entre pop_com et medecins . J'ai appel\u00e9 la dataframe obtenue med_pop_com . med_pop_com = pop_com . merge ( medecins , how = 'left' , left_on = 'Code' , right_on = 'codgeo' ) med_pop_com . head () Ensuite, je n'ai gard\u00e9 que les colonnes utiles et j'ai fait une jointure avec communes . La dataframe obtenue ( med_pop_geo_com ) regroupe donc toutes les donn\u00e9es sur les communes (population, nombre de m\u00e9decins et contours g\u00e9ographiques). med_pop_com = med_pop_com [[ 'Code' , 'Libell\u00e9' , 'Population municipale 2019' , 'nb_medg' ]] med_pop_geo_com = med_pop_com . merge ( communes , how = 'inner' , left_on = 'Code' , right_on = 'INSEE_COM' ) med_pop_geo_com . head () Il restait alors \u00e0 faire la jointure avec les donn\u00e9es sur les bassins de vie. Le probl\u00e8me \u00e9tait que je ne disposais pas de colonnes communes dans les deux tables med_pop_geo_com et bassins . J'ai contourn\u00e9 le probl\u00e8me en faisant une jointure spatiale . Pour faire cette jointure spatiale, j'ai d\u00e9cid\u00e9 de calculer les coordonn\u00e9es du centre g\u00e9ographique de chaque commune. Puis j'ai fait une jointure en s\u00e9lectionnant pour chaque bassin de vie les communes dont le centre se situe dans le bassin. J'ai utilis\u00e9 le param\u00e8tre contains de la m\u00e9thode sjoin du module geopandas . J'ai appel\u00e9 la dataframe finale df_total . # Avec les jointures, on se retrouve avec une dataframe et non une geodataframe. Il faut faire une conversion med_pop_geo_com = gpd . GeoDataFrame ( med_pop_geo_com ) #Cr\u00e9ation des centres de chaque commune med_pop_geo_com [ 'centre' ] = med_pop_geo_com . geometry . centroid #suppression de la colonne geometry med_pop_geo_com = med_pop_geo_com . drop ( 'geometry' , axis = 1 ) #Renommer la colonne centre avec le nom geometry (pour la jointure avec sjoin) med_pop_geo_com . rename ( columns = { 'centre' : 'geometry' }, inplace = True ) # Jointure df_total = gpd . sjoin ( bassins , med_pop_geo_com , how = 'left' , predicate = 'contains' ) df_total . head () Calcul des densit\u00e9s Pour faire le calcul de densit\u00e9 de m\u00e9decins par bassin de vie, il faut conna\u00eetre la population totale de chaque bassin ainsi que son nombre de m\u00e9decins. Puis il n'y a plus qu'\u00e0 faire le quotient de ces quantit\u00e9s et multiplier par 100 000 pour obtenir le nombre de m\u00e9decins pour 100 000 habitants. Enfin, il suffit de faire une nouvelle jointure entre la nouvelle dataframe obtenue et df_total . # Population et nombre de m\u00e9decins par bassin df_par_bassin = df_total . groupby ( by = 'bv2012_code' ) . agg ({ 'nb_medg' : 'sum' , 'Population municipale 2019' : 'sum' }) # Densit\u00e9 de m\u00e9decins (nb pour 100 000 hbts) df_par_bassin [ 'densite' ] = round ( df_par_bassin [ 'nb_medg' ] / df_par_bassin [ 'Population municipale 2019' ] * 100000 , 2 ) # S\u00e9lection des colones de df_total qui serviront par la suite df_total = df_total [[ 'bv2012_name_upper' , 'bv2012_code' , 'geometry' ]] df_par_bassin . rename ( columns = { 'Population municipale 2019' : 'population' }, inplace = True ) # jointure finale (avec suppression des doublons dus \u00e0 la jointure) df_final = df_total . merge ( df_par_bassin , on = 'bv2012_code' , how = 'inner' ) df_final = df_final . drop_duplicates () df_final . head () Cr\u00e9ation de la carte Avant de cr\u00e9er la carte, on peut observer les principaux param\u00e8tres statistiques de la densit\u00e9 de m\u00e9decins. df_final [ 'densite' ] . describe () Remarque On peut remarquer qu'il existe des bassins \u00e0 densit\u00e9 nulle. Apr\u00e8s exploration des donn\u00e9es, on trouve qu'il s'agit des bassins de l'\u00cele d'Yeu et de Arzacq-Arraziguet. Apr\u00e8s une recherche sur internet, il semble exister une maison m\u00e9dicale dans ces bassins mais qui n'a sans-doute pas \u00e9t\u00e9 comptabilis\u00e9e comme m\u00e9decin g\u00e9n\u00e9raliste dans les donn\u00e9es de l'Observatoire des territoires. On peut ensuite faire une carte choropl\u00e8the de ces densit\u00e9s de m\u00e9decins. Pour cela, j'ai regroup\u00e9 les bassins de vie par classe. La d\u00e9limitation des classes s'est faite par quartiles. Puis j'ai utilis\u00e9 les modules geopandas et matplotlib pour cr\u00e9er la carte. import matplotlib.pyplot as plt # Cr\u00e9ation des classes df_final [ 'classe' ] = pd . qcut ( df_final [ 'densite' ], 4 , [ '0 - 63,7' , '63,7 - 80' , '80 - 98,7' , '+98,7' ]) # Cr\u00e9ation d'une dataframe avec les coordonn\u00e9es de villes \u00e0 faire appara\u00eetre sur la carte liste_villes = [{ 'ville' : 'Paris' , 'lat' : 48.85341 , 'long' : 2.3488 }, { 'ville' : 'Marseille' , 'lat' : 43.299999 , 'long' : 5.4 }, { 'ville' : 'Lyon' , 'lat' : 45.7640430 , 'long' : 4.8356590 }, { 'ville' : 'Toulouse' , 'lat' : 43.6046520 , 'long' : 1.4442090 }, { 'ville' : 'Bordeaux' , 'lat' : 44.8377890 , 'long' : - 0.5791800 }, { 'ville' : 'Nantes' , 'lat' : 47.2183710 , 'long' : - 1.5536210 }, { 'ville' : 'Lille' , 'lat' : 50.6292500 , 'long' : 3.0572560 }, { 'ville' : 'Strasbourg' , 'lat' : 48.5734053 , 'long' : 7.7521113 }, { 'ville' : 'Brest' , 'lat' : 48.3903940 , 'long' : - 4.4860760 }, { 'ville' : 'Orl\u00e9ans' , 'lat' : 47.90289 , 'long' : 1.9038900 }, { 'ville' : 'Poitiers' , 'lat' : 46.58333 , 'long' : 0.33333 }, { 'ville' : 'Caen' , 'lat' : 49.18585 , 'long' : - 0.35912 }, { 'ville' : 'Ajaccio' , 'lat' : 41.92723 , 'long' : 8.73462 }] df_villes = pd . DataFrame ( liste_villes ) #Transformation de la dataframe des villes en geodataframe gdf_villes = gpd . GeoDataFrame ( df_villes , geometry = gpd . points_from_xy ( df_villes . long , df_villes . lat )) #cr\u00e9ation de la figure fig , ax = plt . subplots ( 1 , 1 , figsize = ( 15 , 15 )) #Suppression des axes ax . get_xaxis () . set_visible ( False ) ax . get_yaxis () . set_visible ( False ) #legende leg_kwds = { 'title' : 'Nombre de m\u00e9decins pour 100 000 hbts' , 'loc' : 'lower left' , 'fontsize' : 11 , 'title_fontsize' : 11 } #cr\u00e9ation de la carte df_final . plot ( ax = ax , column = 'classe' , cmap = 'OrRd' , alpha = 0.8 , edgecolor = 'black' , linewidth = 0.1 , legend = True , legend_kwds = leg_kwds ) #titre et notes plt . title ( \"Densit\u00e9 de m\u00e9decins g\u00e9n\u00e9ralistes par bassin de vie\" , fontsize = 17 ) plt . figtext ( 0.53 , 0.06 , \"Donn\u00e9es sur la population fran\u00e7aise : INSEE (recensement 2019) \\n Donn\u00e9es sur les m\u00e9decins : Observatoire des territoires (ann\u00e9e 2020) \\n Donn\u00e9es g\u00e9ographiques des bassins de vie : INSEE\" , style = 'italic' , ha = \"center\" , fontsize = 11 , bbox = { \"facecolor\" : \"white\" , \"edgecolor\" : \"white\" , \"alpha\" : 0.5 , \"pad\" : 5 }) # Placement des points ax . scatter ( df_villes . long , df_villes . lat , c = 'black' ) # Placement des noms de villes bbox = dict ( boxstyle = \"round\" , alpha = 0.8 , color = 'white' ) for _ , row in df_villes . iterrows (): ax . text ( row [ 'long' ] + 0.13 , row [ 'lat' ] + 0.13 , row [ 'ville' ], fontsize = 'large' , bbox = bbox ) #Affichage plt . show ()","title":"Densit\u00e9 de m\u00e9decins"},{"location":"projets/medecins/#cartographie-de-la-densite-de-medecins-generalistes-par-bassin-de-vie","text":"","title":"Cartographie de la densit\u00e9 de m\u00e9decins g\u00e9n\u00e9ralistes par bassin de vie"},{"location":"projets/medecins/#competences-mises-en-uvre","text":"Nettoyage de donn\u00e9es Manipulation de donn\u00e9es Jointures de tables (classiques et spatiales) Visualisation de donn\u00e9es (carte choropl\u00e8the)","title":"Comp\u00e9tences mises en \u0153uvre"},{"location":"projets/medecins/#problematique","text":"L'id\u00e9e de ce projet \u00e9tait d'\u00e9tablir en Python une carte choropl\u00e8the indiquant la densit\u00e9 de m\u00e9decins g\u00e9n\u00e9ralistes par bassin de vie en France m\u00e9tropolitaine. Cette page est un r\u00e9sum\u00e9 du travail effectu\u00e9. Le notebook contenant l'ensemble du code se trouve ici. Donn\u00e9es utilis\u00e9es un fichier .xlsx regroupant le nombre de m\u00e9decins g\u00e9n\u00e9ralistes par commune (source : Observatoire des territoires / ann\u00e9e 2020) un fichier .csv contenant la population de chaque commune au recensement de 2019 (source : INSEE) un fichier .geojson regroupant les bassins de vie ainsi que leurs contours g\u00e9ographiques (source : INSEE) un fichier .geojson des diff\u00e9rentes communes fran\u00e7aises avec leurs donn\u00e9es g\u00e9ographiques (source : https://github.com/gregoiredavid/france-geojson ) On peut voir que les donn\u00e9es sur la population et le nombre de m\u00e9decins ne sont pas de la m\u00eame ann\u00e9e. Mais \u00e0 une ann\u00e9e pr\u00e8s, il y a peu de chances qu'il y ait eu une forte variation de ces donn\u00e9es. De m\u00eame en 2022 la population et le nombre de m\u00e9decins des communes a sans-doute un peu chang\u00e9 mais le r\u00e9sultat obtenu pour l'ann\u00e9e 2020 donne une bonne id\u00e9e de la densit\u00e9 de m\u00e9decins par bassin de vie aujourd'hui en 2022. Modules utilis\u00e9s Comme dit aupravant, j'ai utilis\u00e9 le langage Python pour r\u00e9aliser cette carte. Voici les modules que j'ai utilis\u00e9s : le module pandas pour lire les fichiers csv et xlsx, convertir les donn\u00e9es en dataframes et faire des jointures de ces dataframes le module geopandas pour lire les fichiers geojson, convertir les donn\u00e9es en GeoDataFrames ainsi que pour faire des jointures spatiales le module matplotlib pour la cr\u00e9ation de la carte","title":"Probl\u00e9matique"},{"location":"projets/medecins/#nettoyage-des-donnees","text":"La premi\u00e8re op\u00e9ration a \u00e9t\u00e9 de nettoyer les donn\u00e9es. Je ne pr\u00e9senterai pas le code correspondant \u00e0 cette partie ici (se r\u00e9f\u00e9rer au notebook ). Voici ce que j'ai pu observer et faire : Il n'y avait pas de donn\u00e9es manquantes dans les fichiers. Je n'ai rien eu \u00e0 faire de ce c\u00f4t\u00e9. Il n'y avait pas de donn\u00e9es en doublons. Il n'y avait pas de donn\u00e9es abb\u00e9rantes. En revanche, la population de chaque commune \u00e9tait au format str . J'ai donc converti cette variable au format int . Le nombre de communes n'\u00e9tait pas le m\u00eame dans chaque fichier. Apr\u00e8s inspection, je me suis rendu compte que certains fichiers ne contenaient que les donn\u00e9es de France m\u00e9tropolitaine alors que d'autres contenaient aussi des donn\u00e9es de communes d'outre-mer. De plus, dans le fichier sur les contours g\u00e9ographiques des communes, les communes de Paris, Lyon et Marseille \u00e9taient divis\u00e9es selon leurs arrondissements. Il a donc fallu faire en sorte qu'il n'y ait plus qu'une ligne pour chacune de ces villes. Dans le cas contraire, cela aurait pos\u00e9 probl\u00e8me lors des jointures avec les autres tables. J'ai enfin supprim\u00e9 des colonnes dont je n'avais pas besoin dans les dataframes.","title":"Nettoyage des donn\u00e9es"},{"location":"projets/medecins/#jointure-des-tables","text":"Arriv\u00e9 \u00e0 ce stade, les donn\u00e9es utiles se trouvent dans quatre dataframes diff\u00e9rents : pop_com pr\u00e9sentant la population des communes medecins pr\u00e9sentant le nombre de m\u00e9decins par commune communes regroupant les contours g\u00e9ographiques des communes bassins regroupant les contours g\u00e9ographiques des bassins de vie J'ai commenc\u00e9 par faire une jointure entre pop_com et medecins . J'ai appel\u00e9 la dataframe obtenue med_pop_com . med_pop_com = pop_com . merge ( medecins , how = 'left' , left_on = 'Code' , right_on = 'codgeo' ) med_pop_com . head () Ensuite, je n'ai gard\u00e9 que les colonnes utiles et j'ai fait une jointure avec communes . La dataframe obtenue ( med_pop_geo_com ) regroupe donc toutes les donn\u00e9es sur les communes (population, nombre de m\u00e9decins et contours g\u00e9ographiques). med_pop_com = med_pop_com [[ 'Code' , 'Libell\u00e9' , 'Population municipale 2019' , 'nb_medg' ]] med_pop_geo_com = med_pop_com . merge ( communes , how = 'inner' , left_on = 'Code' , right_on = 'INSEE_COM' ) med_pop_geo_com . head () Il restait alors \u00e0 faire la jointure avec les donn\u00e9es sur les bassins de vie. Le probl\u00e8me \u00e9tait que je ne disposais pas de colonnes communes dans les deux tables med_pop_geo_com et bassins . J'ai contourn\u00e9 le probl\u00e8me en faisant une jointure spatiale . Pour faire cette jointure spatiale, j'ai d\u00e9cid\u00e9 de calculer les coordonn\u00e9es du centre g\u00e9ographique de chaque commune. Puis j'ai fait une jointure en s\u00e9lectionnant pour chaque bassin de vie les communes dont le centre se situe dans le bassin. J'ai utilis\u00e9 le param\u00e8tre contains de la m\u00e9thode sjoin du module geopandas . J'ai appel\u00e9 la dataframe finale df_total . # Avec les jointures, on se retrouve avec une dataframe et non une geodataframe. Il faut faire une conversion med_pop_geo_com = gpd . GeoDataFrame ( med_pop_geo_com ) #Cr\u00e9ation des centres de chaque commune med_pop_geo_com [ 'centre' ] = med_pop_geo_com . geometry . centroid #suppression de la colonne geometry med_pop_geo_com = med_pop_geo_com . drop ( 'geometry' , axis = 1 ) #Renommer la colonne centre avec le nom geometry (pour la jointure avec sjoin) med_pop_geo_com . rename ( columns = { 'centre' : 'geometry' }, inplace = True ) # Jointure df_total = gpd . sjoin ( bassins , med_pop_geo_com , how = 'left' , predicate = 'contains' ) df_total . head ()","title":"Jointure des tables"},{"location":"projets/medecins/#calcul-des-densites","text":"Pour faire le calcul de densit\u00e9 de m\u00e9decins par bassin de vie, il faut conna\u00eetre la population totale de chaque bassin ainsi que son nombre de m\u00e9decins. Puis il n'y a plus qu'\u00e0 faire le quotient de ces quantit\u00e9s et multiplier par 100 000 pour obtenir le nombre de m\u00e9decins pour 100 000 habitants. Enfin, il suffit de faire une nouvelle jointure entre la nouvelle dataframe obtenue et df_total . # Population et nombre de m\u00e9decins par bassin df_par_bassin = df_total . groupby ( by = 'bv2012_code' ) . agg ({ 'nb_medg' : 'sum' , 'Population municipale 2019' : 'sum' }) # Densit\u00e9 de m\u00e9decins (nb pour 100 000 hbts) df_par_bassin [ 'densite' ] = round ( df_par_bassin [ 'nb_medg' ] / df_par_bassin [ 'Population municipale 2019' ] * 100000 , 2 ) # S\u00e9lection des colones de df_total qui serviront par la suite df_total = df_total [[ 'bv2012_name_upper' , 'bv2012_code' , 'geometry' ]] df_par_bassin . rename ( columns = { 'Population municipale 2019' : 'population' }, inplace = True ) # jointure finale (avec suppression des doublons dus \u00e0 la jointure) df_final = df_total . merge ( df_par_bassin , on = 'bv2012_code' , how = 'inner' ) df_final = df_final . drop_duplicates () df_final . head ()","title":"Calcul des densit\u00e9s"},{"location":"projets/medecins/#creation-de-la-carte","text":"Avant de cr\u00e9er la carte, on peut observer les principaux param\u00e8tres statistiques de la densit\u00e9 de m\u00e9decins. df_final [ 'densite' ] . describe () Remarque On peut remarquer qu'il existe des bassins \u00e0 densit\u00e9 nulle. Apr\u00e8s exploration des donn\u00e9es, on trouve qu'il s'agit des bassins de l'\u00cele d'Yeu et de Arzacq-Arraziguet. Apr\u00e8s une recherche sur internet, il semble exister une maison m\u00e9dicale dans ces bassins mais qui n'a sans-doute pas \u00e9t\u00e9 comptabilis\u00e9e comme m\u00e9decin g\u00e9n\u00e9raliste dans les donn\u00e9es de l'Observatoire des territoires. On peut ensuite faire une carte choropl\u00e8the de ces densit\u00e9s de m\u00e9decins. Pour cela, j'ai regroup\u00e9 les bassins de vie par classe. La d\u00e9limitation des classes s'est faite par quartiles. Puis j'ai utilis\u00e9 les modules geopandas et matplotlib pour cr\u00e9er la carte. import matplotlib.pyplot as plt # Cr\u00e9ation des classes df_final [ 'classe' ] = pd . qcut ( df_final [ 'densite' ], 4 , [ '0 - 63,7' , '63,7 - 80' , '80 - 98,7' , '+98,7' ]) # Cr\u00e9ation d'une dataframe avec les coordonn\u00e9es de villes \u00e0 faire appara\u00eetre sur la carte liste_villes = [{ 'ville' : 'Paris' , 'lat' : 48.85341 , 'long' : 2.3488 }, { 'ville' : 'Marseille' , 'lat' : 43.299999 , 'long' : 5.4 }, { 'ville' : 'Lyon' , 'lat' : 45.7640430 , 'long' : 4.8356590 }, { 'ville' : 'Toulouse' , 'lat' : 43.6046520 , 'long' : 1.4442090 }, { 'ville' : 'Bordeaux' , 'lat' : 44.8377890 , 'long' : - 0.5791800 }, { 'ville' : 'Nantes' , 'lat' : 47.2183710 , 'long' : - 1.5536210 }, { 'ville' : 'Lille' , 'lat' : 50.6292500 , 'long' : 3.0572560 }, { 'ville' : 'Strasbourg' , 'lat' : 48.5734053 , 'long' : 7.7521113 }, { 'ville' : 'Brest' , 'lat' : 48.3903940 , 'long' : - 4.4860760 }, { 'ville' : 'Orl\u00e9ans' , 'lat' : 47.90289 , 'long' : 1.9038900 }, { 'ville' : 'Poitiers' , 'lat' : 46.58333 , 'long' : 0.33333 }, { 'ville' : 'Caen' , 'lat' : 49.18585 , 'long' : - 0.35912 }, { 'ville' : 'Ajaccio' , 'lat' : 41.92723 , 'long' : 8.73462 }] df_villes = pd . DataFrame ( liste_villes ) #Transformation de la dataframe des villes en geodataframe gdf_villes = gpd . GeoDataFrame ( df_villes , geometry = gpd . points_from_xy ( df_villes . long , df_villes . lat )) #cr\u00e9ation de la figure fig , ax = plt . subplots ( 1 , 1 , figsize = ( 15 , 15 )) #Suppression des axes ax . get_xaxis () . set_visible ( False ) ax . get_yaxis () . set_visible ( False ) #legende leg_kwds = { 'title' : 'Nombre de m\u00e9decins pour 100 000 hbts' , 'loc' : 'lower left' , 'fontsize' : 11 , 'title_fontsize' : 11 } #cr\u00e9ation de la carte df_final . plot ( ax = ax , column = 'classe' , cmap = 'OrRd' , alpha = 0.8 , edgecolor = 'black' , linewidth = 0.1 , legend = True , legend_kwds = leg_kwds ) #titre et notes plt . title ( \"Densit\u00e9 de m\u00e9decins g\u00e9n\u00e9ralistes par bassin de vie\" , fontsize = 17 ) plt . figtext ( 0.53 , 0.06 , \"Donn\u00e9es sur la population fran\u00e7aise : INSEE (recensement 2019) \\n Donn\u00e9es sur les m\u00e9decins : Observatoire des territoires (ann\u00e9e 2020) \\n Donn\u00e9es g\u00e9ographiques des bassins de vie : INSEE\" , style = 'italic' , ha = \"center\" , fontsize = 11 , bbox = { \"facecolor\" : \"white\" , \"edgecolor\" : \"white\" , \"alpha\" : 0.5 , \"pad\" : 5 }) # Placement des points ax . scatter ( df_villes . long , df_villes . lat , c = 'black' ) # Placement des noms de villes bbox = dict ( boxstyle = \"round\" , alpha = 0.8 , color = 'white' ) for _ , row in df_villes . iterrows (): ax . text ( row [ 'long' ] + 0.13 , row [ 'lat' ] + 0.13 , row [ 'ville' ], fontsize = 'large' , bbox = bbox ) #Affichage plt . show ()","title":"Cr\u00e9ation de la carte"},{"location":"projets/seismes/","text":"Carte des s\u00e9ismes dans le monde en 2022 Comp\u00e9tences mises en \u0153uvre Nettoyage et manipulations de donn\u00e9es Visualisation de donn\u00e9es Probl\u00e9matique Ce mini-projet a pour seul but de cr\u00e9er une carte repr\u00e9sentant les s\u00e9ismes de magnitude sup\u00e9rieure \u00e0 4,5 ayant eu lieu en 2022. Le choix de prendre les s\u00e9ismes de magnitude sup\u00e9rieure \u00e0 4,5 a \u00e9t\u00e9 fait pour ne consid\u00e9rer que les s\u00e9ismes les plus importants. On ignore donc ici une grande partie des tremblements de terre car beaucoup ont une magnitude inf\u00e9rieure \u00e0 4,5 voire ne sont m\u00eame pas ressentis par l'homme (magnitude inf\u00e9rieure \u00e0 3). Le notebook contenant le code ainsi que les fichiers de donn\u00e9es sont disponibles ici . Importation des donn\u00e9es et s\u00e9lection des variables utiles Les donn\u00e9es sont issues de l'US Geological Survey (https://www.usgs.gov/programs/earthquake-hazards/earthquakes, 2023). Le site de l'USGS limite le nombre de donn\u00e9es t\u00e9l\u00e9chargeables en un seul fichier. Il a donc \u00e9t\u00e9 n\u00e9cessaire de t\u00e9l\u00e9charger deux fichiers (format geojson) pour obtenir l'ensemble des s\u00e9ismes. La premi\u00e8re \u00e9tape a donc \u00e9t\u00e9 d'importer les deux fichiers sous forme de deux dataframes avec le module geopandas , puis de les concat\u00e9ner. Avant de faire cela, je me suis bien-s\u00fbr assur\u00e9 que les deux dataframes contenaient les m\u00eames colonnes et qu'il n'y avait pas de probl\u00e8mes de donn\u00e9es manquantes dans ces colonnes. import geopandas as gpd import pandas as pd # Importation df_seismes1 = gpd . read_file ( 'seismes01-06.geojson' ) df_seismes2 = gpd . read_file ( 'seismes07-12.geojson' ) # Selection des colonnes df_seismes1 = df_seismes1 [[ 'id' , 'mag' , 'place' , 'time' , 'geometry' ]] df_seismes2 = df_seismes2 [[ 'id' , 'mag' , 'place' , 'time' , 'geometry' ]] # Concatenation df_seismes = pd . concat ([ df_seismes1 , df_seismes2 ]) # Affichage des premi\u00e8res lignes df_seismes . head () On se retrouve avec une seule table dans laquelle on peut v\u00e9rifier qu'il n'y a pas de doublons gr\u00e2ce \u00e0 la m\u00e9thode duplicated du module pandas . \u00c9tant donn\u00e9 que ce n'est pas le cas, passons \u00e0 la suite. Courte description des s\u00e9ismes On peut obtenir un r\u00e9sum\u00e9 statistique de la magnitude des s\u00e9ismes \u00e0 l'aide de la m\u00e9thode describe de pandas . df_seismes [ 'mag' ] . describe () On s'aper\u00e7oit alors qu'en 2022 : Il y a eu 8114 s\u00e9ismes de magnitude sup\u00e9rieure \u00e0 4,5. Le s\u00e9isme le plus important a eu une magnitude de 7,6. 50 % des s\u00e9ismes \u00e9taient de magnitudes inf\u00e9rieures ou \u00e9gales \u00e0 4,7. 75 % des s\u00e9ismes \u00e9taient de magnitudes inf\u00e9rieures ou \u00e9gales \u00e0 4,9. Les s\u00e9ismes de magnitude \u00e9lev\u00e9e sont donc plut\u00f4t rares (heureusement pour nous !). On peut tracer un histogramme de ces s\u00e9ismes pour mieux se rendre compte de leur distribution suivant la magnitude. import matplotlib.pyplot as plt import seaborn as sns # Reindexation car la dataframe poss\u00e8de des lignes avec le m\u00eame index # du \u00e0 la concat\u00e9nation de deux dataframes df_seismes . reset_index ( drop = True , inplace = True ) # Cr\u00e9ation de l'histogramme fig , ax = plt . subplots ( figsize = ( 8 , 4 )) sns . histplot ( x = 'mag' , data = df_seismes , bins = [ 4.5 , 5 , 5.5 , 6 , 6.5 , 7 , 7.5 , 8 ]) plt . xlabel ( 'Magnitude' ) plt . ylabel ( 'Nombre de s\u00e9ismes' ) plt . title ( 'R\u00e9partition des s\u00e9ismes de magnitude sup\u00e9rieure \u00e0 4,5 en 2022' ) plt . show () On voit bien que tr\u00e8s peu de s\u00e9ismes ont une forte magnitude. Carte des s\u00e9ismes Passons \u00e0 ce qui nous int\u00e9resse : la carte des s\u00e9ismes. Pour r\u00e9aliser cette carte, j'ai trac\u00e9 un nuage de points (scatterplot) repr\u00e9sentant chaque s\u00e9isme suivant sa longitude en abscisse et sa latitude en ordonn\u00e9e. Ce nuage de points se fait par dessus un fond de carte du monde. Il nous faut donc importer les donn\u00e9es permettant d'afficher ce fond de carte. L\u00e0-aussi, j'ai utilis\u00e9 un fichier geojson. Pour cr\u00e9er une carte, il faut d\u00e9finir le type de projection du globe terrestre sur un plan. Ici j'ai utilis\u00e9 la projection Mercator. # Chargement du fichier carte = gpd . read_file ( 'world_map.json' ) # Changement de la projection vers la projection Mercator carte = carte . to_crs ( 'EPSG:3395' ) #Affichage du fond de carte carte . plot ( figsize = ( 10 , 10 )) plt . show () Comme on peut le voir, nous avons notre fond de carte. Il ne nous reste plus qu'\u00e0 en modifier l'aspect et repr\u00e9senter les s\u00e9ismes par des disques. Mais pour cela il faut : convertir les donn\u00e9es g\u00e9ospatiales de df_seismes pour avoir le m\u00eame CRS (projection Mercator) r\u00e9cup\u00e9rer la longitude et la latitude de chaque s\u00e9isme dans la colonne geometry On peut ensuite cr\u00e9er la carte en utilisant des points de diff\u00e9rentes couleurs (voir le param\u00e8tre c de la m\u00e9thode scatter de matplotlib qui indique quelles donn\u00e9es utiliser pour la couleur des points et le param\u00e8tre cmap qui donne l'\u00e9chelle de couleurs \u00e0 utiliser) mais aussi de diff\u00e9rentes tailles. Pour cela, j'ai cr\u00e9\u00e9 une liste s des tailles en fonction de la magnitude des s\u00e9ismes. # Changement CRS df_seismes = df_seismes . to_crs ( 'EPSG:3395' ) # Cr\u00e9ation de colonnes correspondant \u00e0 la latitude et la longitude de chaque seisme df_seismes [ 'lat' ] = df_seismes [ 'geometry' ] . y df_seismes [ 'long' ] = df_seismes [ 'geometry' ] . x #Cr\u00e9ation d'une liste de rayons des disques repr\u00e9sentant les s\u00e9ismes en fonction de leur magnitude s = [( mag - 3.5 ) ** 5 for mag in df_seismes [ 'mag' ]] # Cr\u00e9ation de la carte fig , ax = plt . subplots ( 1 , 1 , figsize = ( 25 , 20 ), facecolor = 'white' ) carte_seismes = carte . plot ( ax = ax , color = \"black\" , edgecolor = 'white' , linewidth = 0.2 ) # Suppression des axes ax . get_xaxis () . set_visible ( False ) ax . get_yaxis () . set_visible ( False ) # Couleur de fond gris fonc\u00e9 ax . set_facecolor ( \"#202020\" ) # Repr\u00e9sentation des seismes ax . scatter ( df_seismes [ 'long' ], df_seismes [ 'lat' ], marker = \"o\" , s = s , c = df_seismes [ 'mag' ] , cmap = 'autumn_r' , edgecolors = None , alpha = 0.5 ) plt . title ( \"S\u00e9ismes dans le monde en 2022 (magnitude sup\u00e9rieure \u00e0 4,5)\" , fontsize = 15 ) #Cr\u00e9ation d'une \u00e9chelle de couleur pour la l\u00e9gende sm = plt . cm . ScalarMappable ( cmap = 'autumn_r' ) sm . set_clim ( vmin = 4.5 , vmax = 7.6 ) plt . colorbar ( sm , label = \"Magnitude\" , fraction = 0.02 , shrink = 0.4 , orientation = 'horizontal' , pad = 0.03 ) # Texte indiquant la source des donn\u00e9es plt . figtext ( 0.51 , 0.08 , \"Source : U.S. Geological Survey (https://www.usgs.gov/programs/earthquake-hazards/earthquakes)\" , style = 'italic' , ha = \"center\" , fontsize = 11 , bbox = { \"facecolor\" : \"white\" , \"edgecolor\" : \"white\" , \"pad\" : - 10 }) # Exportation au format png image = plt . gcf () image . savefig ( 'seismes' , bbox_inches = 'tight' , dpi = 200 ) #Affichage plt . show ()","title":"S\u00e9ismes dans le monde en 2022"},{"location":"projets/seismes/#carte-des-seismes-dans-le-monde-en-2022","text":"","title":"Carte des s\u00e9ismes dans le monde en 2022"},{"location":"projets/seismes/#competences-mises-en-uvre","text":"Nettoyage et manipulations de donn\u00e9es Visualisation de donn\u00e9es","title":"Comp\u00e9tences mises en \u0153uvre"},{"location":"projets/seismes/#problematique","text":"Ce mini-projet a pour seul but de cr\u00e9er une carte repr\u00e9sentant les s\u00e9ismes de magnitude sup\u00e9rieure \u00e0 4,5 ayant eu lieu en 2022. Le choix de prendre les s\u00e9ismes de magnitude sup\u00e9rieure \u00e0 4,5 a \u00e9t\u00e9 fait pour ne consid\u00e9rer que les s\u00e9ismes les plus importants. On ignore donc ici une grande partie des tremblements de terre car beaucoup ont une magnitude inf\u00e9rieure \u00e0 4,5 voire ne sont m\u00eame pas ressentis par l'homme (magnitude inf\u00e9rieure \u00e0 3). Le notebook contenant le code ainsi que les fichiers de donn\u00e9es sont disponibles ici .","title":"Probl\u00e9matique"},{"location":"projets/seismes/#importation-des-donnees-et-selection-des-variables-utiles","text":"Les donn\u00e9es sont issues de l'US Geological Survey (https://www.usgs.gov/programs/earthquake-hazards/earthquakes, 2023). Le site de l'USGS limite le nombre de donn\u00e9es t\u00e9l\u00e9chargeables en un seul fichier. Il a donc \u00e9t\u00e9 n\u00e9cessaire de t\u00e9l\u00e9charger deux fichiers (format geojson) pour obtenir l'ensemble des s\u00e9ismes. La premi\u00e8re \u00e9tape a donc \u00e9t\u00e9 d'importer les deux fichiers sous forme de deux dataframes avec le module geopandas , puis de les concat\u00e9ner. Avant de faire cela, je me suis bien-s\u00fbr assur\u00e9 que les deux dataframes contenaient les m\u00eames colonnes et qu'il n'y avait pas de probl\u00e8mes de donn\u00e9es manquantes dans ces colonnes. import geopandas as gpd import pandas as pd # Importation df_seismes1 = gpd . read_file ( 'seismes01-06.geojson' ) df_seismes2 = gpd . read_file ( 'seismes07-12.geojson' ) # Selection des colonnes df_seismes1 = df_seismes1 [[ 'id' , 'mag' , 'place' , 'time' , 'geometry' ]] df_seismes2 = df_seismes2 [[ 'id' , 'mag' , 'place' , 'time' , 'geometry' ]] # Concatenation df_seismes = pd . concat ([ df_seismes1 , df_seismes2 ]) # Affichage des premi\u00e8res lignes df_seismes . head () On se retrouve avec une seule table dans laquelle on peut v\u00e9rifier qu'il n'y a pas de doublons gr\u00e2ce \u00e0 la m\u00e9thode duplicated du module pandas . \u00c9tant donn\u00e9 que ce n'est pas le cas, passons \u00e0 la suite.","title":"Importation des donn\u00e9es et s\u00e9lection des variables utiles"},{"location":"projets/seismes/#courte-description-des-seismes","text":"On peut obtenir un r\u00e9sum\u00e9 statistique de la magnitude des s\u00e9ismes \u00e0 l'aide de la m\u00e9thode describe de pandas . df_seismes [ 'mag' ] . describe () On s'aper\u00e7oit alors qu'en 2022 : Il y a eu 8114 s\u00e9ismes de magnitude sup\u00e9rieure \u00e0 4,5. Le s\u00e9isme le plus important a eu une magnitude de 7,6. 50 % des s\u00e9ismes \u00e9taient de magnitudes inf\u00e9rieures ou \u00e9gales \u00e0 4,7. 75 % des s\u00e9ismes \u00e9taient de magnitudes inf\u00e9rieures ou \u00e9gales \u00e0 4,9. Les s\u00e9ismes de magnitude \u00e9lev\u00e9e sont donc plut\u00f4t rares (heureusement pour nous !). On peut tracer un histogramme de ces s\u00e9ismes pour mieux se rendre compte de leur distribution suivant la magnitude. import matplotlib.pyplot as plt import seaborn as sns # Reindexation car la dataframe poss\u00e8de des lignes avec le m\u00eame index # du \u00e0 la concat\u00e9nation de deux dataframes df_seismes . reset_index ( drop = True , inplace = True ) # Cr\u00e9ation de l'histogramme fig , ax = plt . subplots ( figsize = ( 8 , 4 )) sns . histplot ( x = 'mag' , data = df_seismes , bins = [ 4.5 , 5 , 5.5 , 6 , 6.5 , 7 , 7.5 , 8 ]) plt . xlabel ( 'Magnitude' ) plt . ylabel ( 'Nombre de s\u00e9ismes' ) plt . title ( 'R\u00e9partition des s\u00e9ismes de magnitude sup\u00e9rieure \u00e0 4,5 en 2022' ) plt . show () On voit bien que tr\u00e8s peu de s\u00e9ismes ont une forte magnitude.","title":"Courte description des s\u00e9ismes"},{"location":"projets/seismes/#carte-des-seismes","text":"Passons \u00e0 ce qui nous int\u00e9resse : la carte des s\u00e9ismes. Pour r\u00e9aliser cette carte, j'ai trac\u00e9 un nuage de points (scatterplot) repr\u00e9sentant chaque s\u00e9isme suivant sa longitude en abscisse et sa latitude en ordonn\u00e9e. Ce nuage de points se fait par dessus un fond de carte du monde. Il nous faut donc importer les donn\u00e9es permettant d'afficher ce fond de carte. L\u00e0-aussi, j'ai utilis\u00e9 un fichier geojson. Pour cr\u00e9er une carte, il faut d\u00e9finir le type de projection du globe terrestre sur un plan. Ici j'ai utilis\u00e9 la projection Mercator. # Chargement du fichier carte = gpd . read_file ( 'world_map.json' ) # Changement de la projection vers la projection Mercator carte = carte . to_crs ( 'EPSG:3395' ) #Affichage du fond de carte carte . plot ( figsize = ( 10 , 10 )) plt . show () Comme on peut le voir, nous avons notre fond de carte. Il ne nous reste plus qu'\u00e0 en modifier l'aspect et repr\u00e9senter les s\u00e9ismes par des disques. Mais pour cela il faut : convertir les donn\u00e9es g\u00e9ospatiales de df_seismes pour avoir le m\u00eame CRS (projection Mercator) r\u00e9cup\u00e9rer la longitude et la latitude de chaque s\u00e9isme dans la colonne geometry On peut ensuite cr\u00e9er la carte en utilisant des points de diff\u00e9rentes couleurs (voir le param\u00e8tre c de la m\u00e9thode scatter de matplotlib qui indique quelles donn\u00e9es utiliser pour la couleur des points et le param\u00e8tre cmap qui donne l'\u00e9chelle de couleurs \u00e0 utiliser) mais aussi de diff\u00e9rentes tailles. Pour cela, j'ai cr\u00e9\u00e9 une liste s des tailles en fonction de la magnitude des s\u00e9ismes. # Changement CRS df_seismes = df_seismes . to_crs ( 'EPSG:3395' ) # Cr\u00e9ation de colonnes correspondant \u00e0 la latitude et la longitude de chaque seisme df_seismes [ 'lat' ] = df_seismes [ 'geometry' ] . y df_seismes [ 'long' ] = df_seismes [ 'geometry' ] . x #Cr\u00e9ation d'une liste de rayons des disques repr\u00e9sentant les s\u00e9ismes en fonction de leur magnitude s = [( mag - 3.5 ) ** 5 for mag in df_seismes [ 'mag' ]] # Cr\u00e9ation de la carte fig , ax = plt . subplots ( 1 , 1 , figsize = ( 25 , 20 ), facecolor = 'white' ) carte_seismes = carte . plot ( ax = ax , color = \"black\" , edgecolor = 'white' , linewidth = 0.2 ) # Suppression des axes ax . get_xaxis () . set_visible ( False ) ax . get_yaxis () . set_visible ( False ) # Couleur de fond gris fonc\u00e9 ax . set_facecolor ( \"#202020\" ) # Repr\u00e9sentation des seismes ax . scatter ( df_seismes [ 'long' ], df_seismes [ 'lat' ], marker = \"o\" , s = s , c = df_seismes [ 'mag' ] , cmap = 'autumn_r' , edgecolors = None , alpha = 0.5 ) plt . title ( \"S\u00e9ismes dans le monde en 2022 (magnitude sup\u00e9rieure \u00e0 4,5)\" , fontsize = 15 ) #Cr\u00e9ation d'une \u00e9chelle de couleur pour la l\u00e9gende sm = plt . cm . ScalarMappable ( cmap = 'autumn_r' ) sm . set_clim ( vmin = 4.5 , vmax = 7.6 ) plt . colorbar ( sm , label = \"Magnitude\" , fraction = 0.02 , shrink = 0.4 , orientation = 'horizontal' , pad = 0.03 ) # Texte indiquant la source des donn\u00e9es plt . figtext ( 0.51 , 0.08 , \"Source : U.S. Geological Survey (https://www.usgs.gov/programs/earthquake-hazards/earthquakes)\" , style = 'italic' , ha = \"center\" , fontsize = 11 , bbox = { \"facecolor\" : \"white\" , \"edgecolor\" : \"white\" , \"pad\" : - 10 }) # Exportation au format png image = plt . gcf () image . savefig ( 'seismes' , bbox_inches = 'tight' , dpi = 200 ) #Affichage plt . show ()","title":"Carte des s\u00e9ismes"}]}